# ğŸ§  Chat LLM â€” Train Your Own Chatbot

This is a modular, object-oriented pipeline for training a custom Transformer-based chatbot using the [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst1) dataset from Hugging Face. It includes teacher forcing, validation, early stopping, and an interactive chat interface.

---

## ğŸ“¦ Features

- Custom PyTorch transformer model
- Teacher-forced training
- Validation with early stopping
- Hugging Face tokenizer support (e.g. GPT-2)
- Interactive terminal chatbot
- Clean object-oriented architecture
- Logging with real-time training updates

---

## ğŸ§¹ Folder Structure

```text
chat_llm/
â”œâ”€â”€ config/         # Configuration
â”‚   â””â”€â”€ config.py   # Hyperparameters and constants
â”‚
â”œâ”€â”€ data/           # Dataset wrapper (Hugging Face)
â”‚   â””â”€â”€ dataset.py  # Loads and tokenizes the dataset
â”‚
â”œâ”€â”€ model/          # Transformer + custom layers
â”‚   â”œâ”€â”€ llm_model.py      # Main transformer model
â”‚   â”œâ”€â”€ dynamic_tanh.py   # Custom activation layer
â”‚
â”œâ”€â”€ train/          # Training & validation logic
â”‚   â”œâ”€â”€ trainer.py   # Trainer with early stopping and validation
â”‚   â””â”€â”€ validator.py # (optional) Separate validation logic
â”‚
â”œâ”€â”€ utils/          # Tokenizer abstraction
â”‚   â””â”€â”€ tokenizer.py # Wrapper around Hugging Face tokenizer
â”‚
â”œâ”€â”€ checkpoints/    # Saved model checkpoints (.pt files)
â”‚
â”œâ”€â”€ chatbot.py      # REPL chatbot interface
â”œâ”€â”€ main.py         # Main entry point: train, validate, and launch chatbot
â””â”€â”€ README.md       # Project documentation
```

---

## ğŸš€ Quick Start

### 1. ğŸ”§ Setup

```bash
pip install torch transformers datasets
```

### 2. ğŸ“š Train the Model

```bash
python chat_llm/main.py
```

- Automatically splits validation data
- Uses teacher forcing
- Early stops if no improvement
- Saves best model to `./checkpoints/model.pt`

### 3. ğŸ’¬ Chat!

At the end of training, the chatbot launches automatically.

Example:

```
You: Hello!
Bot: Hello! How can I help you today?
```

---

## âš™ï¸ Configuration

Edit `chat_llm/config/config.py` to adjust:

- Model architecture
- Batch size
- Learning rate
- Max sequence length
- Early stopping patience

---

## ğŸ™ Acknowledgements

- **Dataset:** This project uses the [OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1) dataset, provided by Hugging Face and contributors under a suitable open license.
- **Tokenizer:** GPT-2 tokenizer via Hugging Face Transformers.

---

## ğŸ§  Future Improvements

- Model checkpointing with versioning
- Mixed precision & gradient accumulation
- Web/chat UI with Streamlit or Gradio
- Evaluation metrics (BLEU, perplexity)

---

## ğŸ“œ License

This project is licensed under the **GNU General Public License v3.0 (GPL-3.0)**.

See the [LICENSE](https://www.gnu.org/licenses/gpl-3.0.en.html) file or visit [gnu.org](https://www.gnu.org/licenses/gpl-3.0.html) for full terms.

This project is for educational and research purposes. Respect the license terms of third-party datasets and models.

